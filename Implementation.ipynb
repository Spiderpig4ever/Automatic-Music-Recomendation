{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import choices, sample,randint, choice, shuffle\n",
    "from gensim.matutils import softcossim\n",
    "from math import*\n",
    "from numba import jit\n",
    "\n",
    "import os\n",
    "from os.path import isfile\n",
    "import keras as K\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, Bidirectional, LSTM, Dropout, Activation, GRU\n",
    "from keras.layers import Conv2D,Conv1D,concatenate, MaxPooling2D, Flatten, Embedding, Lambda, Reshape\n",
    "from keras.layers import Input, Dense, TimeDistributed, LSTM, Dropout, Activation\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, Conv2D, BatchNormalization, Lambda\n",
    "from keras.models import model_from_json\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, ReduceLROnPlateau, EarlyStopping\n",
    "from keras import backend as K\n",
    "from keras.utils import np_utils\n",
    "from keras.optimizers import Adam, RMSprop, SGD\n",
    "from keras.models import Sequential\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from keras import regularizers\n",
    "import tensorflow as tf\n",
    "from sklearn.utils import shuffle\n",
    "import time\n",
    "import pickle5 as pickle\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Music Recommendation using Similatiy Learning\n",
    "### Jamie Burns\n",
    "#### University of Witwaterstrand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in the data\n",
    "with open('Data/Library.obj', 'rb') as input:\n",
    "    lib = pickle.load(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(lib.songs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ttv_split(songs):\n",
    "    \"\"\"\n",
    "    songs: Song library\n",
    "    \n",
    "    Retuns: Shuffled array of songs\n",
    "    \"\"\"\n",
    "    random.shuffle(songs)\n",
    "    train = []\n",
    "    test = []\n",
    "    validation = []\n",
    "    trn_count = np.zeros(9)\n",
    "    tst_count = np.zeros(9)\n",
    "    val_count = np.zeros(9)\n",
    "    for s in songs:\n",
    "        l = int(s.label)\n",
    "        if (trn_count[l] < 40) :\n",
    "            train.append(s)\n",
    "            trn_count[l] += 1\n",
    "        elif (tst_count[l] < 5):\n",
    "            test.append(s)\n",
    "            tst_count[l] += 1\n",
    "        else:\n",
    "            validation.append(s)\n",
    "        \n",
    "    return np.array(train), np.array(test), np.array(validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "X_train = np.load(\"train_data.npz\")['arr_0']\n",
    "y_train = np.load(\"train_data.npz\")['arr_1']\n",
    "dict_genres = {'Classical':1, 'Electronic':2, 'Folk':3, 'Jazz':4, \n",
    "                'Rock': 5, 'Old-Time / Historic':6, 'Hip-Hop':7, 'Metal':8 }\n",
    "Y_train = np.zeros(len(y_train))\n",
    "for k in range (len(y_train)):\n",
    "    Y_train[k] = dict_genres[y_train[k]]\n",
    "\"\"\"\n",
    "\n",
    "X_train = np.load('X_new.npy')\n",
    "y_train = np.load('Y_new.npy')\n",
    "\n",
    "X_val = np.load('X_val.npy')\n",
    "y_val = np.load('Y_val.npy')\n",
    "\n",
    "X_test = np.load('X_test.npy')\n",
    "y_test = np.load('Y_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython = True)\n",
    "def _pairwise_distances(embeddings, squared=False):\n",
    "    \"\"\"\n",
    "    Embeddings: Feature embeddings\n",
    "    \"\"\"\n",
    "    #get length of embedding\n",
    "    dim = len(embeddings)\n",
    "    output = np.zeros((dim, dim))\n",
    "    for r in range(dim):\n",
    "        for c in range(dim):\n",
    "            output[r, c] = np.sum(np.square(embeddings[r]-embeddings[c]))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "    _min = 10000\n",
    "    _index = -1\n",
    "    for k in range(len(my_list)):\n",
    "        if (my_list[k] > 0 and my_list[k] < _min):\n",
    "            _min = my_list[k]\n",
    "            _index = k\n",
    "    return _index\n",
    "\n",
    "def get_max(my_list):\n",
    "    _max = -10000\n",
    "    _index = -1\n",
    "    for k in range(len(my_list)):\n",
    "        if (my_list[k] > _max):\n",
    "            _max = my_list[k]\n",
    "            _index = k\n",
    "    return _index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_anchor_positive_triplet_mask(labels):\n",
    "\n",
    "    \"\"\"Return a 2D mask where mask[a, p] is True iff a and p are distinct and have same label.\n",
    "\n",
    "    Args:\n",
    "\n",
    "        labels: tf.int32 `Tensor` with shape [batch_size]\n",
    "    Returns:\n",
    "        mask: tf.bool `Tensor` with shape [batch_size, batch_size]\n",
    "\n",
    "    \"\"\"\n",
    "    # Check that i and j are distinct\n",
    "    indices_equal = tf.cast(tf.eye(tf.shape(labels)[0]), tf.bool)\n",
    "    indices_not_equal = tf.logical_not(indices_equal)\n",
    "    # Check if labels[i] == labels[j]\n",
    "    # Uses broadcasting where the 1st argument has shape (1, batch_size) and the 2nd (batch_size, 1)\n",
    "    labels_equal = tf.equal(tf.expand_dims(labels, 0), tf.expand_dims(labels, 1))\n",
    "    # Combine the two masks\n",
    "    mask = tf.logical_and(indices_not_equal, labels_equal)\n",
    "    return mask\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _get_anchor_negative_triplet_mask(labels):\n",
    "\n",
    "    \"\"\"Return a 2D mask where mask[a, n] is True iff a and n have distinct labels.\n",
    "    Args:\n",
    "\n",
    "        labels: tf.int32 `Tensor` with shape [batch_size]\n",
    "    Returns:\n",
    "        mask: tf.bool `Tensor` with shape [batch_size, batch_size]\n",
    "\n",
    "    \"\"\"\n",
    "    # Check if labels[i] != labels[k]\n",
    "    # Uses broadcasting where the 1st argument has shape (1, batch_size) and the 2nd (batch_size, 1)\n",
    "    labels_equal = tf.equal(tf.expand_dims(labels, 0), tf.expand_dims(labels, 1))\n",
    "    mask = tf.logical_not(labels_equal)\n",
    "    return mask\n",
    "\n",
    "def offline_triplet_loss(labels, embeddings, squared=False):\n",
    "    \"\"\"Build the triplet loss over a batch of embeddings.\n",
    "\n",
    "    For each anchor, we get the hardest positive and hardest negative to form a triplet.\n",
    "\n",
    "    Args:\n",
    "        labels: labels of the batch, of size (batch_size,)\n",
    "        embeddings: tensor of shape (batch_size, embed_dim)\n",
    "        margin: margin for triplet loss\n",
    "        squared: Boolean. If true, output is the pairwise squared euclidean distance matrix.\n",
    "                 If false, output is the pairwise euclidean distance matrix.\n",
    "\n",
    "    Returns:\n",
    "        triplet_loss: scalar tensor containing the triplet loss\n",
    "    \"\"\"\n",
    "    # Get the pairwise distance matrix\n",
    "    distances = _pairwise_distances(embeddings, squared=squared)\n",
    "    pairwise_dist = tf.convert_to_tensor(np.asarray(distances), dtype=tf.float32)\n",
    "    # For each anchor, get the hardest negative\n",
    "    # First, we need to get a mask for every valid negative (they should have different labels)\n",
    "    mask_anchor_negative = _get_anchor_negative_triplet_mask(labels)\n",
    "    mask_anchor_negative = tf.cast(mask_anchor_negative, tf.float32)\n",
    "    anchor_negative_dist = tf.multiply(mask_anchor_negative, pairwise_dist)\n",
    "\n",
    "\n",
    "    return anchor_negative_dist\n",
    "\n",
    "def get_min_index(neg_dist):\n",
    "    hard_neg = []\n",
    "    for k in range(len(neg_dist)):\n",
    "        temp_min = 100\n",
    "        temp_index = -1\n",
    "        for j in range(len(neg_dist)):\n",
    "            temp_neg = neg_dist [k,j]\n",
    "            if temp_neg <= temp_min and temp_neg != 0:\n",
    "                temp_index = j\n",
    "                temp_min = temp_neg\n",
    "        hard_neg.append([k,temp_index])\n",
    "    return hard_neg\n",
    "\n",
    "#Custom batch strategy\n",
    "def batch_method_1(Y_train, database):\n",
    "    #genorate all positvies\n",
    "    mask = _get_anchor_positive_triplet_mask(Y_train)\n",
    "    mask = mask.numpy()\n",
    "    #all the positive pairs in the data set\n",
    "    positives = []\n",
    "    for k in range(len(mask)):\n",
    "        for j  in range(k+1, len(mask)):\n",
    "            if mask[k,j] :\n",
    "                positives.append([k,j,0])\n",
    "    neg_dist = offline_triplet_loss(Y_train, database, squared=False).numpy()\n",
    "    hard_neg = get_min_index(neg_dist)\n",
    "    for k in range(len(positives)):\n",
    "        positives[k][2] = hard_neg[positives[k][0]][1]\n",
    "    return shuffle(positives)\n",
    "\n",
    "#Hard online Triplet minining\n",
    "def batch_method_2(Y_train, database):\n",
    "    #genorate all positvies\n",
    "    positive_mask = tf.cast(_get_anchor_positive_triplet_mask(Y_train), dtype=tf.float32)\n",
    "    #genorate all negatives\n",
    "    negative_mask = tf.cast(_get_anchor_negative_triplet_mask(Y_train), dtype=tf.float32)\n",
    "    #get pairwise distance matrix\n",
    "    _distance_matrix = tf.convert_to_tensor(np.asarray(_pairwise_distances(database, squared=False)), dtype = tf.float32)\n",
    "    positive_distances = tf.multiply(positive_mask, _distance_matrix).numpy() \n",
    "    negative_distances = tf.multiply(negative_mask, _distance_matrix).numpy()\n",
    "    triplets = []\n",
    "    for k in range(len(negative_distances)):\n",
    "        triplets.append([k,get_max(positive_distances[k]),get_min(negative_distances[k])])\n",
    "    return triplets\n",
    "\n",
    "#Semi-online triplet mining\n",
    "def batch_method_3(Y_train, database, margin = 0.4):\n",
    "    #genorate all positvies\n",
    "    positive_mask = tf.cast(_get_anchor_positive_triplet_mask(Y_train), dtype=tf.float32)\n",
    "    #genorate all negatives\n",
    "    negative_mask = tf.cast(_get_anchor_negative_triplet_mask(Y_train), dtype=tf.float32)\n",
    "    #get pairwise distance matrix\n",
    "    _distance_matrix = tf.convert_to_tensor(np.asarray(_pairwise_distances(database, squared=False)), dtype = tf.float32)\n",
    "    positive_distances = tf.multiply(positive_mask, _distance_matrix).numpy() \n",
    "    negative_distances = tf.multiply(negative_mask, _distance_matrix).numpy()\n",
    "    triplets = []\n",
    "    #semi-hard triplet mining\n",
    "    triplets = []\n",
    "    temp = [0,0,0]\n",
    "    for k in range(len(positive_distances)):\n",
    "        for j in range(k+1, len(positive_distances[k])):\n",
    "            if positive_distances[k,j] > 0:\n",
    "                curr_max = 100\n",
    "                for l in range(len(positive_distances)):\n",
    "                    if negative_distances[k,l] < curr_max  and negative_distances[k,l] > positive_distances[k,j] and negative_distances[k,l] < positive_distances[k,j] + margin :\n",
    "                        temp = [k, j, l]\n",
    "                        curr_max = negative_distances[k,l]\n",
    "                if not (curr_max == 100):\n",
    "                    triplets.append(temp)\n",
    "                        \n",
    "                    \n",
    "    \n",
    "    return np.array(triplets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Locate Categories \n",
    "def find_cat(category, index, Y_train):\n",
    "    while True:\n",
    "        index_2 = randint(0, len(Y_train)-1)\n",
    "        if (Y_train[index_2] == category and index_2 != index):\n",
    "            return index_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Genorate triplets \n",
    "def generate(batch_size, X_train, Y_train):\n",
    "    while True:\n",
    "        data, labels = gen_data(X_train, Y_train, batch_size)\n",
    "        yield data, labels\n",
    "        #triplets, labels = get_triplets(Y_train, X_train, batch_size)\n",
    "        #yield triplets, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Triplet loss\n",
    "def get_triplets(Y_train, X_train, batch_size = 1):\n",
    "     #get number of classes\n",
    "    classes = np.unique(Y_train)\n",
    "    n_classes = len(classes)\n",
    "    n_samples, w, h  = X_train.shape\n",
    "    k = 0\n",
    "    count = 0\n",
    "    output = np.zeros(batch_size)\n",
    "    inc = int(batch_size/n_classes)\n",
    "    while k < batch_size:\n",
    "        output[k:k+inc] = classes[count]\n",
    "        k = k+inc\n",
    "        count = count+1\n",
    "    np.random.shuffle(output)\n",
    "    #create the triplet array for songs\n",
    "    triplets =[np.zeros((batch_size, w, h, 1)) for i in range(3)]\n",
    "    #populate the categories\n",
    "    for k in range(batch_size):\n",
    "        category = output[k]\n",
    "        Anchor =  find_cat(category, -1, Y_train)\n",
    "        #Find positive image  \n",
    "        Positive = find_cat(category, Anchor, Y_train)\n",
    "        #Find negative image \n",
    "        while True:\n",
    "            category_2 = sample(classes.tolist(), 1)[0]\n",
    "            if not( category_2 == category ):\n",
    "                break\n",
    "        \n",
    "        Negative = find_cat(category_2, Anchor, Y_train)\n",
    "        triplets[0][k, :, :] = X_train[Anchor].reshape(w, h,1) \n",
    "        triplets[1][k,:,:] = X_train[Positive].reshape(w, h,1)\n",
    "        triplets[2][k,:,:] = X_train[Negative].reshape(w, h,1)\n",
    "    labels = np.zeros(batch_size)\n",
    "\n",
    "    return triplets, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data(X_train):\n",
    "    data_base =[np.zeros((len(X_train), 640, 128, 1)) for i in range(3)]\n",
    "    for k in range(len(X_train)):\n",
    "        data_base[0][k, :, :] = X_train[k].reshape(640, 128,1)\n",
    "    return data_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square_rooted(x):\n",
    " \n",
    "   return round(sqrt(sum([a*a for a in x])),3)\n",
    " \n",
    "def cosine_similarity(x,y):\n",
    "    numerator = sum(a*b for a,b in zip(x,y))\n",
    "    denominator = square_rooted(x)*square_rooted(y)\n",
    "    return numerator/float(denominator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_loss(y_true, y_pred, alpha = 0.4):\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    Implementation of the triplet loss function\n",
    "    Arguments:\n",
    "    y_true -- true labels, required when you define a loss in Keras, you don't need it in this function.\n",
    "    y_pred -- python list containing three objects:\n",
    "            anchor -- the encodings for the anchor data\n",
    "            positive -- the encodings for the positive data (similar to anchor)\n",
    "            negative -- the encodings for the negative data (different from anchor)\n",
    "    Returns:\n",
    "    loss -- real number, value of the loss\n",
    "    \"\"\"\n",
    "\n",
    "    anchor = y_pred[:,0:60]\n",
    "    positive = y_pred[:,60:120]\n",
    "    negative = y_pred[:,120:180]\n",
    "    # distance between the anchor and the positive\n",
    "    pos_dist = K.sum(K.square(anchor-positive),axis=1)\n",
    "    # distance between the anchor and the negative\n",
    "    neg_dist = K.sum(K.square(anchor-negative),axis=1)\n",
    "    # compute loss\n",
    "    basic_loss = pos_dist-neg_dist+alpha\n",
    "    loss = K.maximum(basic_loss,0.0)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#used in method 2\n",
    "def find_cat_online(category, index, Y_train):\n",
    "    while True:\n",
    "        index_2 = randint(0, len(Y_train)-1)\n",
    "        if (Y_train[index_2] == category and not index_2 in index ):\n",
    "            return index_2\n",
    "        \n",
    "def gen_data(X_train, Y_train, batch_size):\n",
    "    n_per_class = int(batch_size/8)\n",
    "    data = np.zeros((batch_size, 640, 128))\n",
    "    count = 0\n",
    "    labels = []\n",
    "    classes = np.unique(Y_train)\n",
    "    for k in classes:\n",
    "        indices = []\n",
    "        indices.append(-1)\n",
    "        for l in range(n_per_class):\n",
    "            curr_index = find_cat_online(k, indices, Y_train)\n",
    "            indices.append(curr_index)\n",
    "            data[count] = X_train[curr_index]\n",
    "            count+=1\n",
    "            labels.append(Y_train[curr_index])\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#used in method 1\n",
    "def get_data(triplets_, X_train):\n",
    "    \n",
    "    #create the triplet array for songs\n",
    "    triplets =[np.zeros((len(triplets_), 640, 128, 1)) for i in range(3)]\n",
    "    #populate the categories\n",
    "    for k in range(len(triplets_)):\n",
    "        triplets[0][k, :, :] = X_train[triplets_[k][0]].reshape(640, 128,1) \n",
    "        triplets[1][k,:,:] = X_train[triplets_[k][1]].reshape(640, 128,1)\n",
    "        triplets[2][k,:,:] = X_train[triplets_[k][2]].reshape(640, 128,1)\n",
    "    return triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nb_filters1=16 \n",
    "nb_filters2=32 \n",
    "nb_filters3=64\n",
    "nb_filters4=128\n",
    "nb_filters5=64\n",
    "ksize = (3,1)\n",
    "pool_size_1= (2,2) \n",
    "pool_size_2= (4,4)\n",
    "pool_size_3 = (4,2)\n",
    "\n",
    "dropout_prob = 0.20\n",
    "dense_size1 = 128\n",
    "lstm_count = 64\n",
    "num_units = 120\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "EPOCH_COUNT = 50\n",
    "L2_regularization = 0.001\n",
    "\n",
    "def conv_recurrent_model_build(model_input):\n",
    "\n",
    "    layer = Input(shape = model_input)\n",
    "    \n",
    "    ### Convolutional blocks\n",
    "    conv_1 = Conv2D(filters = nb_filters1, kernel_size = ksize, strides=1,\n",
    "                      padding= 'valid', activation='relu', name='conv_1')(layer)\n",
    "    pool_1 = MaxPooling2D(pool_size_1)(conv_1)\n",
    "\n",
    "    conv_2 = Conv2D(filters = nb_filters2, kernel_size = ksize, strides=1,\n",
    "                      padding= 'valid', activation='relu', name='conv_2')(pool_1)\n",
    "    pool_2 = MaxPooling2D(pool_size_1)(conv_2)\n",
    "\n",
    "    conv_3 = Conv2D(filters = nb_filters3, kernel_size = ksize, strides=1,\n",
    "                      padding= 'valid', activation='relu', name='conv_3')(pool_2)\n",
    "    pool_3 = MaxPooling2D(pool_size_1)(conv_3)\n",
    "    \n",
    "    \n",
    "    conv_4 = Conv2D(filters = nb_filters4, kernel_size = ksize, strides=1,\n",
    "                      padding= 'valid', activation='relu', name='conv_4')(pool_3)\n",
    "    pool_4 = MaxPooling2D(pool_size_2)(conv_4)\n",
    "    \n",
    "    \n",
    "    conv_5 = Conv2D(filters = nb_filters5, kernel_size = ksize, strides=1,\n",
    "                      padding= 'valid', activation='relu', name='conv_5')(pool_4)\n",
    "    pool_5 = MaxPooling2D(pool_size_2)(conv_5)\n",
    "\n",
    "    flatten1 = Flatten()(pool_5)\n",
    "    ### Recurrent Block\n",
    "    \n",
    "    # Pooling layer\n",
    "    pool_lstm1 = MaxPooling2D(pool_size_3, name = 'pool_lstm')(layer)\n",
    "    \n",
    "    # Embedding layer\n",
    "\n",
    "    squeezed = Lambda(lambda x: K.squeeze(x, axis= -1))(pool_lstm1)\n",
    "#     flatten2 = K.squeeze(pool_lstm1, axis = -1)\n",
    "#     dense1 = Dense(dense_size1)(flatten)\n",
    "    \n",
    "    # Bidirectional GRU\n",
    "    lstm = Bidirectional(GRU(lstm_count))(squeezed)  #default merge mode is concat\n",
    "    \n",
    "    # Concat Output\n",
    "    concat = concatenate([flatten1, lstm], axis=-1, name ='concat')\n",
    "    \n",
    "    ## Softmax Output\n",
    "    output = Dense(60, activation = 'sigmoid', name='preds')(concat)\n",
    "    \n",
    "    ## Drop layer right at the end\n",
    "    #normalize = Lambda(lambda x: tf.math.l2_normalize(x, axis=1))(output)\n",
    "    model_output = output\n",
    "    model = Model(layer, model_output)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def baseline_model(model_input):\n",
    "\n",
    "    layer = Input(shape = model_input)\n",
    "    \n",
    "    ### Convolutional blocks\n",
    "    conv_1 = Conv2D(filters = nb_filters1, kernel_size = ksize, strides=1,\n",
    "                      padding= 'valid', activation='relu', name='conv_1')(layer)\n",
    "    pool_1 = MaxPooling2D(pool_size_1)(conv_1)\n",
    "\n",
    "    conv_2 = Conv2D(filters = nb_filters2, kernel_size = ksize, strides=1,\n",
    "                      padding= 'valid', activation='relu', name='conv_2')(pool_1)\n",
    "    pool_2 = MaxPooling2D(pool_size_1)(conv_2)\n",
    "\n",
    "    conv_3 = Conv2D(filters = nb_filters3, kernel_size = ksize, strides=1,\n",
    "                      padding= 'valid', activation='relu', name='conv_3')(pool_2)\n",
    "    pool_3 = MaxPooling2D(pool_size_1)(conv_3)\n",
    "    \n",
    "    \n",
    "    conv_4 = Conv2D(filters = nb_filters4, kernel_size = ksize, strides=1,\n",
    "                      padding= 'valid', activation='relu', name='conv_4')(pool_3)\n",
    "    pool_4 = MaxPooling2D(pool_size_2)(conv_4)\n",
    "    \n",
    "    \n",
    "    conv_5 = Conv2D(filters = nb_filters5, kernel_size = ksize, strides=1,\n",
    "                      padding= 'valid', activation='relu', name='conv_5')(pool_4)\n",
    "    pool_5 = MaxPooling2D(pool_size_2)(conv_5)\n",
    "\n",
    "    flatten1 = Flatten()(pool_5)\n",
    "    ### Recurrent Block\n",
    "    \n",
    "    # Pooling layer\n",
    "    pool_lstm1 = MaxPooling2D(pool_size_3, name = 'pool_lstm')(layer)\n",
    "    \n",
    "    # Embedding layer\n",
    "\n",
    "    squeezed = Lambda(lambda x: K.squeeze(x, axis= -1))(pool_lstm1)\n",
    "#     flatten2 = K.squeeze(pool_lstm1, axis = -1)\n",
    "#     dense1 = Dense(dense_size1)(flatten)\n",
    "    \n",
    "    # Bidirectional GRU\n",
    "    lstm = Bidirectional(GRU(lstm_count))(squeezed)  #default merge mode is concat\n",
    "    \n",
    "    # Concat Output\n",
    "    concat = concatenate([flatten1, lstm], axis=-1, name ='concat')\n",
    "    \n",
    "    ## Softmax Output\n",
    "    dense_1 = Dense(60, activation = 'sigmoid', name='dense_1')(concat)\n",
    "    output = Dense(8, activation = 'softmax')(dense_1)\n",
    "    \n",
    "    ## Drop layer right at the end\n",
    "    #normalize = Lambda(lambda x: tf.math.l2_normalize(x, axis=1))(output)\n",
    "    model_output = output\n",
    "    model = Model(layer, model_output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Metrics\n",
    "def square_rooted(x):\n",
    " \n",
    "   return round(sqrt(sum([a*a for a in x])),3)\n",
    " \n",
    "def cosine_similarity(x,y):\n",
    "    numerator = sum(a*b for a,b in zip(x,y))\n",
    "    denominator = square_rooted(x)*square_rooted(y)\n",
    "    return numerator/float(denominator)\n",
    "#Results\n",
    "def get_MRR(mfcc, labels):\n",
    "    MRR = 0\n",
    "    for k in range(len(labels)):\n",
    "        first_count = 0\n",
    "        dist = []\n",
    "        for j in range(len(labels)):\n",
    "            if not( j == k):\n",
    "                dist.append([cosine_similarity(mfcc[j], mfcc[k]), labels[j]])\n",
    "        curr = pd.DataFrame(dist, columns=['distance', 'label'])\n",
    "        l = curr.sort_values(by = 'distance', ascending = False).head(n = 9).label.values\n",
    "        count = 0\n",
    "        for p in range(len(l)):\n",
    "            if l[p] == labels[k]:\n",
    "                first_count = p\n",
    "                break\n",
    "        first_count += 1\n",
    "        \n",
    "        if not (first_count == 0):\n",
    "            MRR += 1 / first_count\n",
    "    return MRR/len(mfcc)\n",
    "\n",
    "def get_MRR_per_class(mfcc, labels):\n",
    "    output = np.zeros(len(np.unique(labels)))\n",
    "    for k in range(len(labels)):\n",
    "        first_count = 0\n",
    "        dist = []\n",
    "        for j in range(len(labels)):\n",
    "            if not( j == k):\n",
    "                dist.append([cosine_similarity(mfcc[j], mfcc[k]), labels[j]])\n",
    "        curr = pd.DataFrame(dist, columns=['distance', 'label'])\n",
    "        l = curr.sort_values(by = 'distance', ascending = False).head(n = 9).label.values\n",
    "        count = 0\n",
    "        for p in range(len(l)):\n",
    "            if l[p] == labels[k]:\n",
    "                first_count = p\n",
    "                break\n",
    "        first_count += 1\n",
    "         \n",
    "        if not (first_count == 0):\n",
    "            output[int(labels[k] - 1)] += 1/first_count\n",
    "    return output/10\n",
    "\n",
    "def get_MAP_per_class(mfcc, labels):\n",
    "    output = np.zeros(len(np.unique(labels)))\n",
    "    for k in range(len(labels)):\n",
    "        AP = 0\n",
    "        dist = []\n",
    "        for j in range(len(labels)):\n",
    "            if not( j == k):\n",
    "                dist.append([cosine_similarity(mfcc[j], mfcc[k]), labels[j]])\n",
    "        curr = pd.DataFrame(dist, columns=['distance', 'label'])\n",
    "        l = curr.sort_values(by = 'distance', ascending = False).head(n = 9).label.values\n",
    "        p_k, mask  = precision_k(l, labels[k])\n",
    "        if  (sum(mask) == 0):\n",
    "            AP = 0\n",
    "        else:\n",
    "            AP = sum(p_k*mask) / sum(mask)\n",
    "        output[int(labels[k] - 1)] += AP\n",
    "    return output/10\n",
    "\n",
    "def precision_k(labels, target, num = 9):\n",
    "    mask = (labels == target)*1\n",
    "    output = np.zeros( len(mask) )\n",
    "    count  = 0\n",
    "    for k in range( num ):\n",
    "        count += mask[k]\n",
    "        output[k] = count / (k+1)\n",
    "    return output, mask\n",
    "\n",
    "def mean_pk (mfcc, labels, num):\n",
    "    mk = 0\n",
    "    for k in range(len(labels)):\n",
    "        dist = []\n",
    "        for j in range(len(labels)):\n",
    "            if not( j == k):\n",
    "                dist.append([cosine_similarity(mfcc[j], mfcc[k]), labels[j]])\n",
    "        curr = pd.DataFrame(dist, columns=['distance', 'label'])\n",
    "        l = curr.sort_values(by = 'distance', ascending = False).head(n = 9).label.values\n",
    "        count = 0\n",
    "        pk, _ = precision_k(l, labels[k], num)\n",
    "        mk += pk[num - 1]\n",
    "    return mk/len(labels)\n",
    "\n",
    "def get_MAP(mfcc, labels):\n",
    "    MAP = 0\n",
    "    for k in range(len(labels)):\n",
    "        AP = 0\n",
    "        dist = []\n",
    "        for j in range(len(labels)):\n",
    "            if not( j == k):\n",
    "                dist.append([cosine_similarity(mfcc[j], mfcc[k]), labels[j]])\n",
    "        curr = pd.DataFrame(dist, columns=['distance', 'label'])\n",
    "        l = curr.sort_values(by = 'distance', ascending = False).head(n = 9).label.values\n",
    "        p_k, mask  = precision_k(l, labels[k])\n",
    "        if  (sum(mask) == 0):\n",
    "            AP = 0\n",
    "        else:\n",
    "            AP = sum(p_k*mask) / sum(mask)\n",
    "        MAP += AP\n",
    "    return MAP/len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X_test):\n",
    "    json_file = open('model.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    loaded_model.load_weights(\"model_best.h5\")\n",
    "    test_base =[np.zeros((len(X_test), 640, 128, 1)) for i in range(3)]\n",
    "    for k in range(len(X_test)):\n",
    "        test_base[0][k, :, :] = X_test[k].reshape(640, 128,1)\n",
    "    testbase = loaded_model.predict(test_base)[:,0:60]\n",
    "    return testbase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    n_frequency = 128 \n",
    "    n_frames = 640\n",
    "    #specify input and output dimensions\n",
    "    input_shape = (n_frames, n_frequency, 1)\n",
    "    model_input = Input(input_shape, name='input')\n",
    "    base_network = conv_recurrent_model_build(input_shape) \n",
    "    anchor_in = Input(input_shape)\n",
    "    pos_in = Input(input_shape)\n",
    "    neg_in = Input(input_shape)\n",
    "    anchor_out = base_network(anchor_in)\n",
    "    pos_out = base_network(pos_in)\n",
    "    neg_out = base_network(neg_in)\n",
    "    merged_vector = concatenate([anchor_out, pos_out, neg_out], axis=-1)\n",
    "    #compile model with tiplet loss (Offline)\n",
    "    model = Model(inputs=[anchor_in, pos_in, neg_in], outputs=merged_vector)\n",
    "    model.compile(optimizer=Adam(),loss=triplet_loss)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_train(X_train, Y_train, EPOCHS = 5):\n",
    "    #get the model\n",
    "    model = get_model()\n",
    "    BATCH_SIZE = 40\n",
    "    NUM_BATCHES = 50\n",
    "    for E in range(EPOCHS):  \n",
    "        #Network tings\n",
    "        load_bar = ''\n",
    "        t_loss = 0\n",
    "        #print('Epoch ', E)\n",
    "        for B in range(NUM_BATCHES):\n",
    "            train_input, train_labels = get_triplets(Y_train, X_train, BATCH_SIZE)\n",
    "            loss = model.train_on_batch(train_input, train_labels)\n",
    "            #print('Iteration', B , load_bar, 'Loss:', loss, end='\\r')\n",
    "            load_bar += '|'\n",
    "            t_loss += loss\n",
    "            model_json = model.to_json()\n",
    "            with open(\"model.json\", \"w\") as json_file:\n",
    "                json_file.write(model_json)\n",
    "                # serialize weights to HDF5\n",
    "                model.save_weights(\"rb.h5\")\n",
    "        #print(\"\")\n",
    "        #print(\"Loss\", t_loss/NUM_BATCHES)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X_train, Y_train, X_val, Y_val, batch_strategy):\n",
    "    if not (batch_strategy == None):\n",
    "        model = pre_train(X_train, Y_train, 5)\n",
    "    else:\n",
    "        model = get_model()\n",
    "        \n",
    "    #print(\"Part 2 (Batch Semi-Hard Loss)\") \n",
    "    EPOCHS = 10\n",
    "    BATCH_SIZE = 80\n",
    "    NUM_BATCHES = 50\n",
    "    min_val_loss = 10\n",
    "    count = 0\n",
    "    for E in range(EPOCHS):\n",
    "        flag = 0\n",
    "        #Network tings\n",
    "        load_bar = ''\n",
    "        t_loss = 0\n",
    "        tv_loss = 0\n",
    "        #print('Epoch ', E)\n",
    "        for B in range(NUM_BATCHES):\n",
    "            #Genarate batch\n",
    "            x_train, y_train = gen_data(X_train, Y_train, 40)\n",
    "            x_val, y_val = gen_data(X_val, Y_val, 40)\n",
    "            if not(batch_strategy == None):\n",
    "                #get embeddings\n",
    "                train_embeddings = model.predict(format_data(x_train))[:,0:60]\n",
    "                val_embeddings = model.predict(format_data(x_val))[:,0:60]\n",
    "                #create triplets\n",
    "                train_triplets = batch_strategy(y_train, train_embeddings)\n",
    "                val_triplets = batch_strategy(y_val, val_embeddings)\n",
    "                #fetch input data\n",
    "                train_input  = get_data(train_triplets, x_train)\n",
    "                val_input  = get_data(val_triplets, x_val)\n",
    "                #create dummy labels\n",
    "                train_labels = np.zeros(len(train_triplets))\n",
    "                val_labels = np.zeros(len(val_triplets))\n",
    "            else:\n",
    "                train_input, train_labels = get_triplets(Y_train, X_train, BATCH_SIZE)\n",
    "                val_input, val_labels = get_triplets(Y_val, X_val, BATCH_SIZE)\n",
    "                \n",
    "            #train model\n",
    "            loss = model.train_on_batch(train_input, train_labels)\n",
    "            val_loss = model.test_on_batch(val_input, val_labels)\n",
    "            #save model weights\n",
    "            if (val_loss < min_val_loss):\n",
    "                model_json = model.to_json()\n",
    "                with open(\"model.json\", \"w\") as json_file:\n",
    "                    json_file.write(model_json)\n",
    "                    # serialize weights to HDF5\n",
    "                    model.save_weights(\"model_best.h5\")\n",
    "                min_val_loss = val_loss\n",
    "                flag = 1\n",
    "            #output loss\n",
    "            #print('Iteration', B , load_bar, 'Validation Loss:', val_loss, end='\\r')\n",
    "            if flag == 0:\n",
    "                count += 1\n",
    "            else:\n",
    "                count = 0\n",
    "            if count == 2:\n",
    "                break\n",
    "                \n",
    "            load_bar += '|'\n",
    "            t_loss += loss\n",
    "            tv_loss += val_loss\n",
    "        #print(\"\")\n",
    "        #print(\"Loss\", t_loss/NUM_BATCHES)\n",
    "        #print(\"validation Loss\", tv_loss/NUM_BATCHES)\n",
    "    return  min_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_base(X_train, y_train, X_val, y_val, epochs = 200, batch_size = 32):\n",
    "    n_frequency = 128 \n",
    "    n_frames = 640\n",
    "    #Correct variables for training\n",
    "    X_train = X_train.reshape(len(X_train), n_frames, n_frequency, 1)\n",
    "    y_train = pd.get_dummies(y_train).values\n",
    "    X_val = X_val.reshape(len(X_val), n_frames, n_frequency, 1)\n",
    "    y_val = pd.get_dummies(y_val).values\n",
    "    #define and compile model\n",
    "    input_shape = (n_frames, n_frequency, 1)\n",
    "    model  = baseline_model(input_shape)\n",
    "    model.compile(optimizer=Adam(),loss=\"categorical_crossentropy\")\n",
    "    filepath=\"best_model.h5\"\n",
    "    checkpoint = EarlyStopping(monitor='val_loss', patience=5,  mode='min')\n",
    "    callbacks_list = [checkpoint]\n",
    "    model.fit(X_train, y_train, validation_data = (X_val, y_val), epochs = epochs, batch_size = batch_size, callbacks = callbacks_list, verbose = 0 )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(lib = None, batch_method = None, iters = 0):\n",
    "    \n",
    "    MRR = []\n",
    "    MAP = []\n",
    "    P_1 = []\n",
    "    P_5 = []\n",
    "    P_9 = []\n",
    "    MAP_PC = []\n",
    "    MRR_PC = []\n",
    "    for k in range(iters):\n",
    "        print(\"iteration:\", k)\n",
    "        #get all the data\n",
    "        train, test ,val = get_ttv_split(lib.songs)\n",
    "        X_train = np.array([s.MEL for s in train])\n",
    "        y_train = np.array([s.label for s in train])\n",
    "        X_val = np.array([s.MEL for s in val])\n",
    "        y_val = np.array([s.label for s in val])\n",
    "        X_test = np.array([s.MEL for s in test])\n",
    "        X_test = np.concatenate([X_test, X_val])\n",
    "        y_test = np.array([s.label for s in test])\n",
    "        y_test = np.concatenate([y_test, y_val])\n",
    "        #Train the model\n",
    "        train_model(X_train, y_train, X_val, y_val, batch_method)\n",
    "        tb = predict(X_test)        \n",
    "        #get the results\n",
    "        MRR.append(get_MRR(tb, y_test))\n",
    "        MAP.append(get_MAP(tb, y_test))\n",
    "        P_1.append(mean_pk(tb, y_test, 1))\n",
    "        P_5.append(mean_pk(tb, y_test, 5))\n",
    "        P_9.append(mean_pk(tb, y_test, 9))\n",
    "        #Per class values\n",
    "        MAP_PC.append(get_MAP_per_class(tb, y_test))\n",
    "        MRR_PC.append(get_MRR_per_class(tb, y_test))\n",
    "    return MRR, MAP, P_1, P_5, P_9, MAP_PC, MRR_PC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 1144\n"
     ]
    }
   ],
   "source": [
    "def get_results_mfcc(lib = None, batch_method = None, iters = 0):\n",
    "    \n",
    "    MRR = []\n",
    "    MAP = []\n",
    "    P_1 = []\n",
    "    P_5 = []\n",
    "    P_9 = []\n",
    "    MAP_PC = []\n",
    "    MRR_PC = []\n",
    "    for k in range(iters):\n",
    "        print(\"iteration:\", k)\n",
    "        #get all the data\n",
    "        train, test ,val = get_ttv_split(lib.songs)\n",
    "        X_val = np.array([s.MFCC for s in val])\n",
    "        y_val = np.array([s.label for s in val])\n",
    "        X_test = np.array([s.MFCC for s in test])\n",
    "        X_test = np.concatenate([X_test, X_val])\n",
    "        y_test = np.array([s.label for s in test])\n",
    "        y_test = np.concatenate([y_test, y_val])\n",
    "        MRR.append(get_MRR(X_test, y_test))\n",
    "        MAP.append(get_MAP(X_test, y_test))\n",
    "        P_1.append(mean_pk(X_test, y_test, 1))\n",
    "        P_5.append(mean_pk(X_test, y_test, 5))\n",
    "        P_9.append(mean_pk(X_test, y_test, 9))\n",
    "        #Per class values\n",
    "        MAP_PC.append(get_MAP_per_class(X_test, y_test))\n",
    "        MRR_PC.append(get_MRR_per_class(X_test, y_test))\n",
    "    return MRR, MAP, P_1, P_5, P_9, MAP_PC, MRR_PC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0\n",
      "iteration: 1\n",
      "iteration: 2\n",
      "iteration: 3\n",
      "iteration: 4\n",
      "iteration: 5\n",
      "iteration: 6\n",
      "iteration: 7\n",
      "iteration: 8\n",
      "iteration: 9\n"
     ]
    }
   ],
   "source": [
    "MRR, MAP, P_1, P_5, P_9, MAP_PC, MRR_PC = get_results_mfcc(lib = lib, batch_method = None, iters = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = np.load('BL.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 1\n",
      "MAP_PC: 0.77\n",
      "Var: 0.015\n",
      "Class 2\n",
      "MAP_PC: 0.73\n",
      "Var: 0.009\n",
      "Class 3\n",
      "MAP_PC: 0.82\n",
      "Var: 0.008\n",
      "Class 4\n",
      "MAP_PC: 0.62\n",
      "Var: 0.024\n",
      "Class 5\n",
      "MAP_PC: 1.0\n",
      "Var: 0.0\n",
      "Class 6\n",
      "MAP_PC: 0.69\n",
      "Var: 0.008\n",
      "Class 7\n",
      "MAP_PC: 0.65\n",
      "Var: 0.006\n",
      "Class 8\n",
      "MAP_PC: 1.0\n",
      "Var: 0.0\n",
      "Class 1\n",
      "MRR_PC: 0.87\n",
      "Var: 0.011\n",
      "Class 2\n",
      "MRR_PC: 0.81\n",
      "Var: 0.01\n",
      "Class 3\n",
      "MRR_PC: 0.9\n",
      "Var: 0.004\n",
      "Class 4\n",
      "MRR_PC: 0.78\n",
      "Var: 0.017\n",
      "Class 5\n",
      "MRR_PC: 1.0\n",
      "Var: 0.0\n",
      "Class 6\n",
      "MRR_PC: 0.79\n",
      "Var: 0.008\n",
      "Class 7\n",
      "MRR_PC: 0.75\n",
      "Var: 0.007\n",
      "Class 8\n",
      "MRR_PC: 1.0\n",
      "Var: 0.0\n"
     ]
    }
   ],
   "source": [
    "for file in ['MAP_PC', 'MRR_PC']:\n",
    "    for k in range(8):\n",
    "        print('Class {}'.format(k+1))\n",
    "        mean = np.round( np.mean(res[file], axis = 0), 2)[k]\n",
    "        print('{}: {}'.format(file,  mean) )\n",
    "        var = np.round( np.var(res[file], axis = 0), 3)[k]\n",
    "        print('Var: {}'.format(var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_res(Y_test, tsne_results, title):\n",
    "    \"\"\"\n",
    "    Y_test: Class labels\n",
    "    tsne_results: 2D reduced feature\n",
    "    title: name of the plot\n",
    "    \n",
    "    Return: None\n",
    "    \"\"\"\n",
    "    classes = np.unique(Y_test)\n",
    "    fig = plt.figure(figsize=(20, 16))\n",
    "    ax1 = fig.add_subplot(111)\n",
    "    markers = [\"o\", \"v\", \"s\", \"P\", \"d\", \"x\", \"p\", \"1\"]\n",
    "    count  = 0\n",
    "    for c in classes:\n",
    "        mask  = Y_test == c\n",
    "        curr_class = tsne_results[mask]\n",
    "        ax1.scatter(curr_class[:, 0], curr_class[:, 1], marker = markers[count], label = f\"Class {c}\", s  = 200)\n",
    "        count += 1\n",
    "    plt.legend(loc  = \"upper left\", prop = {'size':20})\n",
    "    plt.title(title, fontsize = 30)\n",
    "    plt.axis(False)\n",
    "    plt.show()\n",
    "\n",
    "def plot_embeddings(Y_test, testbase, title = None):\n",
    "    \"\"\"\n",
    "    Y_test: class labels\n",
    "    testbase: Feature embeddings\n",
    "    title: title of plot\n",
    "    \n",
    "    returns: none\n",
    "    \"\"\"\n",
    "    #Dimensionality reduction\n",
    "    tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\n",
    "    tsne_results = tsne.fit_transform(testbase)\n",
    "    plot_res(Y_test, tsne_results, title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
